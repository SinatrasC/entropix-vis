{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "LG1ZrQ7WPysg",
        "e2IdvdlSltd9",
        "F0fWAb83ghC1",
        "mx72XH3OiYTB",
        "muWfAoylOo0y",
        "WuT59jyTlNOj",
        "eC0q4zYfMwzR",
        "IYjjOvqElY1R",
        "QIPiutqxM6Yr"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs and initial imports"
      ],
      "metadata": {
        "id": "LG1ZrQ7WPysg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tyro\n",
        "!pip install tiktoken\n",
        "!pip install blobfile"
      ],
      "metadata": {
        "id": "YfNdtczYLK8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZd9EBLmgKU6"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple, Optional, Tuple\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tyro"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Model ID and Token"
      ],
      "metadata": {
        "id": "-v7mNYwHnPJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = 'meta-llama/Llama-3.2-1B-Instruct'\n",
        "TOKEN = '' #you need to enter your huggingface token here to download the weights"
      ],
      "metadata": {
        "id": "oHaZYRqAnRPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "e2IdvdlSltd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple\n",
        "params = {\n",
        "  \"dim\": 2048,\n",
        "  \"n_layers\": 16,\n",
        "  \"n_heads\": 32,\n",
        "  \"n_kv_heads\": 8,\n",
        "  \"vocab_size\": 128256,\n",
        "  \"ffn_dim_multiplier\": 1.5,\n",
        "  \"multiple_of\": 256,\n",
        "  \"norm_eps\": 1e-05,\n",
        "  \"rope_theta\": 500000.0,\n",
        "  \"use_scaled_rope\": True,\n",
        "  \"max_seq_len\": 4096\n",
        "}\n",
        "\n",
        "\n",
        "class ModelParams(NamedTuple):\n",
        "  n_layers: int\n",
        "  n_local_heads: int\n",
        "  n_local_kv_heads: int\n",
        "  head_dim: int\n",
        "  max_seq_len: int\n",
        "  rope_theta: float\n",
        "  use_scaled_rope: bool\n",
        "\n",
        "\n",
        "LLAMA_1B_PARAMS = ModelParams(\n",
        "  n_layers=params[\"n_layers\"],\n",
        "  n_local_heads=params[\"n_heads\"],\n",
        "  n_local_kv_heads=params[\"n_kv_heads\"],\n",
        "  head_dim=params[\"dim\"] // params[\"n_heads\"],\n",
        "  max_seq_len=params[\"max_seq_len\"],\n",
        "  rope_theta=params[\"rope_theta\"],\n",
        "  use_scaled_rope=params[\"use_scaled_rope\"]\n",
        ")"
      ],
      "metadata": {
        "id": "RHxP8Bd1lvo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Weights"
      ],
      "metadata": {
        "id": "F0fWAb83ghC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NHcDsZzshQji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import ml_dtypes\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from pathlib import Path\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from unittest.mock import patch\n",
        "from transformers.dynamic_module_utils import get_imports"
      ],
      "metadata": {
        "id": "flDeKnQlhS1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_key(in_key: str):\n",
        "    out_key = in_key.replace('.weight', '')\n",
        "    if out_key.startswith('model.'):\n",
        "        out_key = out_key.replace('model.', '')\n",
        "        if out_key.endswith('input_layernorm'):\n",
        "            out_key = out_key.replace('input_layernorm', 'attention_norm')\n",
        "        elif out_key.endswith('mlp.down_proj'):\n",
        "            out_key = out_key.replace('mlp.down_proj', 'feed_forward.w2')\n",
        "        elif out_key.endswith('mlp.gate_proj'):\n",
        "            out_key = out_key.replace('mlp.gate_proj', 'feed_forward.w1')\n",
        "        elif out_key.endswith('mlp.up_proj'):\n",
        "            out_key = out_key.replace('mlp.up_proj', 'feed_forward.w3')\n",
        "        elif out_key.endswith('post_attention_layernorm'):\n",
        "            out_key = out_key.replace('post_attention_layernorm', 'ffn_norm')\n",
        "        elif out_key.endswith('self_attn.k_proj'):\n",
        "            out_key = out_key.replace('self_attn.k_proj', 'attention.wk')\n",
        "        elif out_key.endswith('self_attn.o_proj'):\n",
        "            out_key = out_key.replace('self_attn.o_proj', 'attention.wo')\n",
        "        elif out_key.endswith('self_attn.q_proj'):\n",
        "            out_key = out_key.replace('self_attn.q_proj', 'attention.wq')\n",
        "        elif out_key.endswith('self_attn.v_proj'):\n",
        "            out_key = out_key.replace('self_attn.v_proj', 'attention.wv')\n",
        "        elif out_key.endswith('down_proj'):\n",
        "            out_key = out_key.replace('down_proj', 'w2')\n",
        "        elif out_key.endswith('gate_proj'):\n",
        "            out_key = out_key.replace('gate_proj', 'w1')\n",
        "        elif out_key.endswith('up_proj'):\n",
        "            out_key = out_key.replace('up_proj', 'w3')\n",
        "        elif out_key == 'embed_tokens':\n",
        "            out_key = 'tok_embeddings'\n",
        "        elif out_key == 'norm':\n",
        "            out_key = 'norm'\n",
        "        else:\n",
        "            print(f\"Don't know how to handle {in_key=}\")\n",
        "    elif out_key == 'lm_head':\n",
        "        out_key = 'output'\n",
        "    else:\n",
        "        print(f\"Don't know how to handle {in_key=}\")\n",
        "    return f'{out_key}.weight'\n",
        "\n",
        "\n",
        "def reverse_permute(tensor: torch.Tensor, n_heads: int = 32, dim1:int = 4096, dim2: int = 4096) -> torch.Tensor:\n",
        "    return tensor.view(n_heads, 2, dim1 // n_heads // 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n",
        "\n",
        "\n",
        "def fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
        "    \"\"\"Work around for https://huggingface.co/microsoft/phi-1_5/discussions/72.\"\"\"\n",
        "    if not str(filename).endswith(\"/modeling_deepseek.py\"):\n",
        "        return get_imports(filename)\n",
        "    imports = get_imports(filename)\n",
        "    imports.remove(\"flash_attn\")\n",
        "    return imports\n",
        "\n",
        "def download_weights(model_id: str = MODEL_ID, out_dir: Path = Path('weights/1B-Instruct')):\n",
        "    device = torch.device(\"cpu\")\n",
        "    if not out_dir.exists():\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with patch(\"transformers.dynamic_module_utils.get_imports\", fixed_get_imports):\n",
        "      hf_model = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype=torch.bfloat16, offload_folder=\"/tmp/offload\", token=TOKEN, device_map='cpu')\n",
        "      with torch.no_grad():\n",
        "        state_dict = hf_model.state_dict()\n",
        "        for hf_name, param in state_dict.items():\n",
        "            print(f' {hf_name}: {param.shape=}')\n",
        "            name = translate_key(hf_name)\n",
        "            param = param.cpu()\n",
        "            if name.endswith('wq.weight'):\n",
        "                param = reverse_permute(param, n_heads=32, dim1=2048, dim2=2048)  # 1B\n",
        "            elif name.endswith('wk.weight'): #wk.weight\n",
        "                param = reverse_permute(param, n_heads=8, dim1=512, dim2=2048)  # 1B\n",
        "            else:\n",
        "                pass\n",
        "            bf16_np_out = param.cpu().view(dtype=torch.uint16).numpy().view(ml_dtypes.bfloat16)\n",
        "            bf16_out = jnp.asarray(bf16_np_out, dtype=jnp.bfloat16).reshape(*param.shape)\n",
        "            print(f'Writing {hf_name} as {name} to {out_dir}/{name}.npy')\n",
        "            jnp.save(f'{out_dir}/{name}.npy', bf16_out)\n",
        "    del hf_model\n",
        "    del state_dict\n",
        "\n",
        "download_weights()\n",
        "jax.clear_caches()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "u3lTK6HWhbFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Tokeniser"
      ],
      "metadata": {
        "id": "n6jjIsOZP4oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --header=\"Authorization: Bearer {TOKEN}\" \"https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/original/tokenizer.model?download=true\" -O tokenizer.model\n"
      ],
      "metadata": {
        "id": "4pfn5LaVQYNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Weights"
      ],
      "metadata": {
        "id": "mx72XH3OiYTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, NamedTuple\n",
        "import torch\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "import ml_dtypes\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "class LayerWeights(NamedTuple):\n",
        "  wq: torch.Tensor\n",
        "  wk: torch.Tensor\n",
        "  wv: torch.Tensor\n",
        "  wo: torch.Tensor\n",
        "  w1: torch.Tensor\n",
        "  w2: torch.Tensor\n",
        "  w3: torch.Tensor\n",
        "  ffn_norm: torch.Tensor\n",
        "  attention_norm: torch.Tensor\n",
        "\n",
        "class XfmrWeights(NamedTuple):\n",
        "  tok_embeddings: torch.Tensor\n",
        "  norm: torch.Tensor\n",
        "  output: torch.Tensor\n",
        "  layer_weights: List[LayerWeights]\n",
        "\n",
        "def load_weights(ckpt_dir: Path = Path('weights/1B-Instruct'), n_layers: int = 16):\n",
        "  w = {}\n",
        "  layer_weights = []\n",
        "  with torch.inference_mode():\n",
        "    for file in ckpt_dir.glob(\"*.npy\"):\n",
        "      name = '.'.join(str(file).split('/')[-1].split('.')[:-1])\n",
        "      jax_weight = jnp.load(file=file, mmap_mode='r', allow_pickle=True)\n",
        "      #print(f'JAX output (first 30): {jax_weight.flatten()[:30]}')\n",
        "      np_weight = np.array(jax_weight).astype(np.float32)\n",
        "      weight = torch.from_numpy(np_weight).to(torch.bfloat16).to(device)\n",
        "      w[name] = weight.to(device)\n",
        "    for i in range(n_layers):\n",
        "      layer_weights.append(LayerWeights(\n",
        "        wq=w[f'layers.{i}.attention.wq.weight'],\n",
        "        wk=w[f'layers.{i}.attention.wk.weight'],\n",
        "        wv=w[f'layers.{i}.attention.wv.weight'],\n",
        "        wo=w[f'layers.{i}.attention.wo.weight'],\n",
        "        w1=w[f'layers.{i}.feed_forward.w1.weight'],\n",
        "        w2=w[f'layers.{i}.feed_forward.w2.weight'],\n",
        "        w3=w[f'layers.{i}.feed_forward.w3.weight'],\n",
        "        ffn_norm=w[f'layers.{i}.ffn_norm.weight'],\n",
        "        attention_norm=w[f'layers.{i}.attention_norm.weight'],\n",
        "      ))\n",
        "\n",
        "    xfmr_weights = XfmrWeights(\n",
        "      tok_embeddings=w['tok_embeddings.weight'],\n",
        "      norm=w['norm.weight'],\n",
        "      output=w['output.weight'],\n",
        "      layer_weights=layer_weights\n",
        "    )\n",
        "\n",
        "    return xfmr_weights\n",
        "\n",
        "#xfmr_weights = load_weights()"
      ],
      "metadata": {
        "id": "2WdLNnGTicBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokeniser"
      ],
      "metadata": {
        "id": "muWfAoylOo0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from logging import getLogger\n",
        "from pathlib import Path\n",
        "from typing import (\n",
        "  AbstractSet,\n",
        "  cast,\n",
        "  Collection,\n",
        "  Dict,\n",
        "  Iterator,\n",
        "  List,\n",
        "  Literal,\n",
        "  Optional,\n",
        "  Sequence,\n",
        "  Union,\n",
        ")\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from tiktoken.load import load_tiktoken_bpe\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "\n",
        "\n",
        "# The tiktoken tokenizer can handle <=400k chars without\n",
        "# pyo3_runtime.PanicException.\n",
        "TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
        "\n",
        "# https://github.com/openai/tiktoken/issues/195\n",
        "# Here we iterate over subsequences and split if we exceed the limit\n",
        "# of max consecutive non-whitespace or whitespace characters.\n",
        "MAX_NO_WHITESPACES_CHARS = 25_000\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "  \"\"\"\n",
        "  Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n",
        "  \"\"\"\n",
        "\n",
        "  special_tokens: Dict[str, int]\n",
        "\n",
        "  num_reserved_special_tokens = 256\n",
        "\n",
        "  pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
        "\n",
        "  def __init__(self, model_path: str):\n",
        "    \"\"\"\n",
        "    Initializes the Tokenizer with a Tiktoken model.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): The path to the Tiktoken model file.\n",
        "    \"\"\"\n",
        "    assert os.path.isfile(model_path), model_path\n",
        "\n",
        "    mergeable_ranks = load_tiktoken_bpe(model_path)\n",
        "    num_base_tokens = len(mergeable_ranks)\n",
        "    special_tokens = [\n",
        "      '<|begin_of_text|>',\n",
        "      '<|end_of_text|>',\n",
        "      '<|reserved_special_token_0|>',\n",
        "      '<|reserved_special_token_1|>',\n",
        "      '<|finetune_right_pad_id|>',\n",
        "      '<|step_id|>',\n",
        "      '<|start_header_id|>',\n",
        "      '<|end_header_id|>',\n",
        "      '<|eom_id|>',  # end of message\n",
        "      '<|eot_id|>',  # end of turn\n",
        "      '<|python_tag|>',\n",
        "    ]\n",
        "    reserved_tokens = [\n",
        "      f'<|reserved_special_token_{2 + i}|>'\n",
        "      for i in range(self.num_reserved_special_tokens - len(special_tokens))\n",
        "    ]\n",
        "    special_tokens = special_tokens + reserved_tokens\n",
        "\n",
        "    self.special_tokens = {token: num_base_tokens + i for i, token in enumerate(special_tokens)}\n",
        "    self.model = tiktoken.Encoding(\n",
        "      name=Path(model_path).name,\n",
        "      pat_str=self.pat_str,\n",
        "      mergeable_ranks=mergeable_ranks,\n",
        "      special_tokens=self.special_tokens,\n",
        "    )\n",
        "\n",
        "    self.n_words: int = num_base_tokens + len(special_tokens)\n",
        "    # BOS / EOS token IDs\n",
        "    self.bos_id: int = self.special_tokens['<|begin_of_text|>']\n",
        "    self.eos_id: int = self.special_tokens['<|end_of_text|>']\n",
        "    self.eot_id: int = self.special_tokens['<|eot_id|>']\n",
        "    self.eom_id: int = self.special_tokens['<|eom_id|>']\n",
        "    self.python_tag_id = self.special_tokens['<|python_tag|>']\n",
        "    self.pad_id: int = self.special_tokens['<|finetune_right_pad_id|>']\n",
        "    self.stop_tokens = [\n",
        "      self.special_tokens['<|eom_id|>'],\n",
        "      self.special_tokens['<|eot_id|>'],\n",
        "    ]\n",
        "\n",
        "  def encode(\n",
        "    self,\n",
        "    s: str,\n",
        "    *,\n",
        "    bos: bool,\n",
        "    eos: bool,\n",
        "    allowed_special: Optional[Union[Literal['all'], AbstractSet[str]]] = None,\n",
        "    disallowed_special: Union[Literal['all'], Collection[str]] = (),\n",
        "  ) -> List[int]:\n",
        "    \"\"\"\n",
        "    Encodes a string into a list of token IDs.\n",
        "\n",
        "    Args:\n",
        "        s (str): The input string to be encoded.\n",
        "        bos (bool): Whether to prepend the beginning-of-sequence token.\n",
        "        eos (bool): Whether to append the end-of-sequence token.\n",
        "        allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n",
        "        disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n",
        "\n",
        "    Returns:\n",
        "        list[int]: A list of token IDs.\n",
        "\n",
        "    By default, setting disallowed_special=() encodes a string by ignoring\n",
        "    special tokens. Specifically:\n",
        "    - Setting `disallowed_special` to () will cause all text corresponding\n",
        "      to special tokens to be encoded as natural text (insteading of raising\n",
        "      an error).\n",
        "    - Setting `allowed_special` to \"all\" will treat all text corresponding\n",
        "      to special tokens to be encoded as special tokens.\n",
        "    \"\"\"\n",
        "    if allowed_special is None:\n",
        "      allowed_special = set()\n",
        "    assert isinstance(s, str)\n",
        "\n",
        "    substrs = (\n",
        "      substr\n",
        "      for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
        "      for substr in self._split_whitespaces_or_nonwhitespaces(\n",
        "        s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
        "      )\n",
        "    )\n",
        "    t: List[int] = []\n",
        "    for substr in substrs:\n",
        "      t.extend(\n",
        "        self.model.encode(\n",
        "          substr,\n",
        "          allowed_special=allowed_special,\n",
        "          disallowed_special=disallowed_special,\n",
        "        )\n",
        "      )\n",
        "    if bos:\n",
        "      t.insert(0, self.bos_id)\n",
        "    if eos:\n",
        "      t.append(self.eos_id)\n",
        "    return t\n",
        "\n",
        "  def decode(self, t: Sequence[int]) -> str:\n",
        "    \"\"\"\n",
        "    Decodes a list of token IDs into a string.\n",
        "\n",
        "    Args:\n",
        "        t (List[int]): The list of token IDs to be decoded.\n",
        "\n",
        "    Returns:\n",
        "        str: The decoded string.\n",
        "    \"\"\"\n",
        "    # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n",
        "    return self.model.decode(cast(List[int], t))\n",
        "\n",
        "  @staticmethod\n",
        "  def _split_whitespaces_or_nonwhitespaces(s: str, max_consecutive_slice_len: int) -> Iterator[str]:\n",
        "    \"\"\"\n",
        "    Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n",
        "    consecutive whitespaces or consecutive non-whitespaces.\n",
        "    \"\"\"\n",
        "    current_slice_len = 0\n",
        "    current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
        "    slice_start = 0\n",
        "\n",
        "    for i in range(len(s)):\n",
        "      is_now_space = s[i].isspace()\n",
        "\n",
        "      if current_slice_is_space ^ is_now_space:\n",
        "        current_slice_len = 1\n",
        "        current_slice_is_space = is_now_space\n",
        "      else:\n",
        "        current_slice_len += 1\n",
        "        if current_slice_len > max_consecutive_slice_len:\n",
        "          yield s[slice_start:i]\n",
        "          slice_start = i\n",
        "          current_slice_len = 1\n",
        "    yield s[slice_start:]"
      ],
      "metadata": {
        "id": "uG484QmzOqsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KVCache"
      ],
      "metadata": {
        "id": "WuT59jyTlNOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Device selection, tree is like first apple silicion, then cuda, fallback is cpu.\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#print(f\"Using device: {device}\")\n",
        "\n",
        "class KVCache(nn.Module):\n",
        "    def __init__(self, layers: int, bsz: int, max_seq_len: int, kv_heads: int, head_dim: int):\n",
        "        super(KVCache, self).__init__()\n",
        "        # Initialize k and v as buffers to ensure they're part of the module state\n",
        "        self.register_buffer(\n",
        "            'k',\n",
        "            torch.zeros(\n",
        "                (layers, bsz, max_seq_len, kv_heads, head_dim),\n",
        "                dtype=torch.bfloat16,\n",
        "                device=device\n",
        "            )\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            'v',\n",
        "            torch.zeros(\n",
        "                (layers, bsz, max_seq_len, kv_heads, head_dim),\n",
        "                dtype=torch.bfloat16,\n",
        "                device=device\n",
        "            )\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def new(cls, layers: int, bsz: int, max_seq_len: int, kv_heads: int, head_dim: int) -> 'KVCache':\n",
        "        \"\"\"Creates a new KVCache instance with initialized k and v tensors.\"\"\"\n",
        "        return cls(layers, bsz, max_seq_len, kv_heads, head_dim)\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        xk: torch.Tensor,\n",
        "        xv: torch.Tensor,\n",
        "        layer_idx: int,\n",
        "        cur_pos: int,\n",
        "        n_rep: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Updates the cache with new key and value tensors.\n",
        "\n",
        "        Args:\n",
        "            xk (torch.Tensor): New key tensor to insert. Shape should align with (bsz, insert_len, kv_heads, head_dim).\n",
        "            xv (torch.Tensor): New value tensor to insert. Shape should align with (bsz, insert_len, kv_heads, head_dim).\n",
        "            layer_idx (int): The index of the layer to update.\n",
        "            cur_pos (int): The current position in the sequence to start inserting.\n",
        "            n_rep (int): The number of times to repeat the keys and values along the sequence dimension.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]:\n",
        "                - keys: Updated or repeated keys tensor.\n",
        "                - values: Updated or repeated values tensor.\n",
        "        \"\"\"\n",
        "        # Ensure xk and xv have the correct device and dtype\n",
        "        xk = xk.to(self.k.dtype)\n",
        "        xv = xv.to(self.v.dtype)\n",
        "\n",
        "        # Update the k and v tensors in the specified layer and position\n",
        "        insert_len = xk.size(1)  # Assuming xk shape is (bsz, insert_len, kv_heads, head_dim)\n",
        "        self.k[layer_idx, :, cur_pos:cur_pos+insert_len, :, :] = xk\n",
        "        self.v[layer_idx, :, cur_pos:cur_pos+insert_len, :, :] = xv\n",
        "\n",
        "        if cur_pos == 0:\n",
        "            # If inserting at the beginning, repeat the new keys and values\n",
        "            keys = xk.repeat_interleave(n_rep, dim=2)\n",
        "            values = xv.repeat_interleave(n_rep, dim=2)\n",
        "        else:\n",
        "            # Otherwise, repeat the existing keys and values from the cache\n",
        "            keys = self.k[layer_idx].repeat_interleave(n_rep, dim=2)\n",
        "            values = self.v[layer_idx].repeat_interleave(n_rep, dim=2)\n",
        "\n",
        "        return keys, values, self\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Resets the k and v caches to zeros.\"\"\"\n",
        "        self.k.zero_()\n",
        "        self.v.zero_()\n"
      ],
      "metadata": {
        "id": "FoE8AuSDlPtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Stats"
      ],
      "metadata": {
        "id": "eC0q4zYfMwzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Device selection, tree is like first apple silicion, then cuda, fallback is cpu.\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "from typing import NamedTuple\n",
        "\n",
        "class AttnStats(NamedTuple):\n",
        "    entropy: torch.Tensor  # (bsz, n_layers, num_heads)\n",
        "    varentropy: torch.Tensor  # (bsz, n_layers, num_heads)\n",
        "    n_layers: int\n",
        "    n_heads: int\n",
        "\n",
        "    @classmethod\n",
        "    def new(cls, bsz: int, n_layers: int, n_heads: int) -> 'AttnStats':\n",
        "        return cls(\n",
        "            entropy=torch.zeros((bsz, n_layers, n_heads), dtype=torch.float32, device=device),\n",
        "            varentropy=torch.zeros((bsz, n_layers, n_heads), dtype=torch.float32, device=device),\n",
        "            n_layers=n_layers,\n",
        "            n_heads=n_heads\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def avg_entropy(self):\n",
        "        return self.entropy.sum(dim=-1, keepdim=False)  # Average across heads\n",
        "\n",
        "    @property\n",
        "    def std_error(self):\n",
        "        return torch.sqrt(torch.mean(self.varentropy)) / (self.n_heads * self.n_layers)\n",
        "\n",
        "    def update(self, scores: torch.Tensor, layer_idx: int):\n",
        "        # scores shape: (bsz, n_heads, seqlen, n_words)\n",
        "        probs = torch.nn.functional.softmax(scores, dim=-1)\n",
        "        new_entropy = -torch.sum(torch.where(probs > 0, probs * torch.log(probs), torch.tensor(0.0)), dim=-1)\n",
        "        new_varentropy = torch.sum(probs * (torch.log(probs) + new_entropy.unsqueeze(-1))**2, dim=-1)\n",
        "\n",
        "        # Update entropy and varentropy tensors\n",
        "        self.entropy[:, layer_idx, :] = new_entropy\n",
        "        self.varentropy[:, layer_idx, :] = new_varentropy\n",
        "\n",
        "        return self"
      ],
      "metadata": {
        "id": "y-x9GMsYMsy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "IYjjOvqElY1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEFAULT_MASK_VALUE = -0.7 * float(torch.finfo(torch.float32).max)\n",
        "\n",
        "# Device selection, tree is like first apple silicion, then cuda, fallback is cpu.\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#print(f\"Using device: {device}\")\n",
        "\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "def rms_norm(x: torch.Tensor, w: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "  return w * (x * torch.rsqrt(torch.pow(x, 2).mean(-1, keepdim=True) + eps))\n",
        "\n",
        "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor, dtype: torch.dtype = torch.float32) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    reshape_xq = xq.float().reshape(*xq.shape[:-1], -1, 2)\n",
        "    reshape_xk = xk.float().reshape(*xk.shape[:-1], -1, 2)\n",
        "    xq_ = torch.complex(reshape_xq[..., 0], reshape_xq[..., 1])\n",
        "    xk_ = torch.complex(reshape_xk[..., 0], reshape_xk[..., 1])\n",
        "    xq_out = xq_ * freqs_cis.unsqueeze(0).unsqueeze(2)\n",
        "    xk_out = xk_ * freqs_cis.unsqueeze(0).unsqueeze(2)\n",
        "    xq_out = torch.stack((xq_out.real, xq_out.imag), dim=-1).reshape(*xq_out.shape[:-1], -1)\n",
        "    xk_out = torch.stack((xk_out.real, xk_out.imag), dim=-1).reshape(*xk_out.shape[:-1], -1)\n",
        "    return xq_out.to(dtype), xk_out.to(dtype)\n",
        "\n",
        "def attention(x: torch.Tensor, layer_weights: LayerWeights, model_params, cur_pos: int, layer_idx: int, freqs_cis: torch.Tensor, kvcache: KVCache, attn_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, KVCache, torch.Tensor]:\n",
        "    # Check if x is 2D or 3D and adjust accordingly\n",
        "    if x.dim() == 2:\n",
        "        bsz = 1\n",
        "        seq_len, dim = x.shape\n",
        "        x = x.unsqueeze(0)  # Add batch dimension\n",
        "    else:\n",
        "        bsz, seq_len, dim = x.shape\n",
        "\n",
        "    n_rep = model_params.n_local_heads // model_params.n_local_kv_heads\n",
        "    xq = F.linear(x, layer_weights.wq).view(bsz, seq_len, model_params.n_local_heads, model_params.head_dim)\n",
        "    xk = F.linear(x, layer_weights.wk).view(bsz, seq_len, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "    xv = F.linear(x, layer_weights.wv).view(bsz, seq_len, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "    xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis, dtype=xq.dtype)\n",
        "    keys, values, kvcache = kvcache.update(xk, xv, layer_idx, cur_pos, n_rep)\n",
        "    xq = xq.permute(0, 2, 1, 3)  # (bs, n_heads, seqlen, head_dim)\n",
        "    keys = keys.permute(0, 2, 3, 1)  # (bs, n_heads, head_dim, cache_len + seqlen)\n",
        "    values = values.permute(0, 2, 1, 3)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
        "    scores = torch.matmul(xq, keys)\n",
        "    pre_scores = scores / math.sqrt(model_params.head_dim)\n",
        "    scores = pre_scores.to(torch.float32)  # Always do attention softmax at float32\n",
        "    if cur_pos == 0:\n",
        "        scores = scores + attn_mask\n",
        "    mask = torch.where(scores != 0.0, scores, DEFAULT_MASK_VALUE)\n",
        "    padded_logits = torch.where((mask >= DEFAULT_MASK_VALUE * 0.5), scores, DEFAULT_MASK_VALUE)\n",
        "    scores = F.softmax(padded_logits, dim=-1).to(x.dtype)\n",
        "    output = torch.matmul(scores.to(values.dtype), values)\n",
        "    output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
        "    out = F.linear(output, layer_weights.wo)\n",
        "\n",
        "    # If input was 2D, remove the batch dimension from the output\n",
        "    if x.dim() == 2:\n",
        "        out = out.squeeze(0)\n",
        "\n",
        "    return out, kvcache, pre_scores\n",
        "\n",
        "def feed_forward(x: torch.Tensor, layer_weights: LayerWeights) -> torch.Tensor:\n",
        " return F.linear(F.silu(F.linear(x, layer_weights.w1)) * F.linear(x, layer_weights.w3), layer_weights.w2)\n",
        "\n",
        "def xfmr(xfmr_weights: XfmrWeights, model_params: ModelParams, tokens: torch.Tensor, cur_pos: int, freqs_cis: torch.Tensor, kvcache: KVCache, attn_mask: Optional[torch.Tensor]=None) -> Tuple[torch.Tensor, KVCache, torch.Tensor, AttnStats]:\n",
        "    h = xfmr_weights.tok_embeddings[tokens]\n",
        "    attn_stats = AttnStats.new(\n",
        "        bsz=tokens.shape[0],\n",
        "        n_layers=model_params.n_layers,\n",
        "        n_heads=model_params.n_local_heads\n",
        "    )\n",
        "    for i in range(model_params.n_layers):\n",
        "        norm_x = rms_norm(h, xfmr_weights.layer_weights[i].attention_norm)\n",
        "        h_attn, kvcache, scores = attention(norm_x, xfmr_weights.layer_weights[i], model_params, cur_pos, i, freqs_cis, kvcache, attn_mask=attn_mask)\n",
        "        attn_stats = attn_stats.update(scores[:,:,-1,:], i)\n",
        "        h = h + h_attn\n",
        "        h = h + feed_forward(rms_norm(h, xfmr_weights.layer_weights[i].ffn_norm), xfmr_weights.layer_weights[i])\n",
        "    logits = F.linear(rms_norm(h, xfmr_weights.norm), xfmr_weights.output)\n",
        "    return logits, kvcache, scores, attn_stats"
      ],
      "metadata": {
        "id": "oQe0q_Jzlap2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampler"
      ],
      "metadata": {
        "id": "AKbwXIUTNR2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple, Dict\n",
        "from enum import Enum\n",
        "\n",
        "# Device selection\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "LN_2 = 0.69314718056  # ln(2) = 1.0 / LOG2_E\n",
        "\n",
        "class SamplerState(Enum):\n",
        "    FLOWING = \"Flowing with unspoken intent\"\n",
        "    TREADING = \"Treading carefully, asking clarifying questions\"\n",
        "    EXPLORING = \"Exploring forks in the path\"\n",
        "    RESAMPLING = \"Resampling in the mist\"\n",
        "    ADAPTIVE = \"Adaptive Sampling\"\n",
        "\n",
        "def calculate_varentropy_logsoftmax(logits: torch.Tensor, axis: int = -1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Calculate the entropy and varentropy of the probability distribution using logsoftmax.\"\"\"\n",
        "    log_probs = F.log_softmax(logits, dim=axis)\n",
        "    probs = torch.exp(log_probs)\n",
        "    entropy = -torch.sum(probs * log_probs, dim=axis) / LN_2  # Convert to base-2\n",
        "    varentropy = torch.sum(probs * (log_probs / LN_2 + entropy.unsqueeze(-1))**2, dim=axis)\n",
        "    return entropy, varentropy\n",
        "\n",
        "def multinomial_sample_one(probs_sort: torch.Tensor, generator: torch.Generator) -> torch.Tensor:\n",
        "    \"\"\"Samples one token from a multinomial distribution with sorted probabilities.\"\"\"\n",
        "    q = torch.rand(probs_sort.shape, generator=generator, device=probs_sort.device)\n",
        "    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(torch.int32)\n",
        "\n",
        "def _sample(logits: torch.Tensor, temperature: float, top_p: float, top_k: int, min_p: float, generator: torch.Generator = None) -> torch.Tensor:\n",
        "    bsz = logits.shape[0]\n",
        "    logit = logits[:, -1]\n",
        "    probs = F.softmax(logit / temperature, dim=-1)\n",
        "\n",
        "    # Apply min_p sampling\n",
        "    if min_p > 0.0:\n",
        "        p_max = torch.max(probs, dim=-1, keepdim=True).values\n",
        "        indices_to_remove = probs < (min_p * p_max)\n",
        "        logit = torch.where(indices_to_remove, torch.full_like(logit, float('-inf')), logit)\n",
        "        probs = F.softmax(logit, dim=-1)\n",
        "\n",
        "    # Apply top-k sampling\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, k=min(top_k, probs.shape[-1]))\n",
        "    probs_sort = torch.flip(top_k_probs, dims=[-1])\n",
        "    probs_idx = torch.flip(top_k_indices, dims=[-1])\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    # Apply top-p sampling\n",
        "    mask = torch.where(probs_sum - probs_sort > top_p, torch.tensor(1.0, device=device), torch.tensor(0.0, device=device))\n",
        "    probs_sort = probs_sort * (1 - mask)\n",
        "    probs_sort = probs_sort / torch.sum(probs_sort, dim=-1, keepdim=True)\n",
        "    next_token = multinomial_sample_one(probs_sort, generator)\n",
        "    next_token_g = torch.gather(probs_idx, -1, next_token.reshape(bsz, 1).to(torch.int64))\n",
        "    return next_token_g.to(torch.int32)\n",
        "\n",
        "def calculate_metrics(logits: torch.Tensor, attention_scores: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "    entropy, varentropy = calculate_varentropy_logsoftmax(logits)\n",
        "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "    attn_entropy = -torch.sum(attention_probs * torch.log2(torch.clamp(attention_probs, 1e-10, 1.0)), dim=-1)\n",
        "    attn_varentropy = torch.var(attn_entropy, dim=1)\n",
        "\n",
        "    attn_varentropy = torch.where(torch.isnan(attn_varentropy), torch.zeros_like(attn_varentropy), attn_varentropy)\n",
        "    mean_attention = torch.mean(attention_probs, dim=1)\n",
        "    agreement = torch.mean(torch.abs(attention_probs - mean_attention.unsqueeze(1)), dim=(1, 2))\n",
        "\n",
        "    interaction_strength = torch.mean(torch.abs(attention_scores), dim=(1, 2, 3))\n",
        "\n",
        "    return {\n",
        "        \"logits_entropy\": torch.mean(entropy),\n",
        "        \"logits_varentropy\": torch.mean(varentropy),\n",
        "        \"attn_entropy\": torch.mean(attn_entropy),\n",
        "        \"attn_varentropy\": torch.mean(attn_varentropy),\n",
        "        \"agreement\": torch.mean(agreement),\n",
        "        \"interaction_strength\": interaction_strength\n",
        "    }\n",
        "\n",
        "class SamplerConfig:\n",
        "    def __init__(self):\n",
        "        self.temperature = 0.666\n",
        "        self.top_p = 0.90\n",
        "        self.top_k = 27\n",
        "        self.min_p = 0.03\n",
        "\n",
        "        self.low_logits_entropy_threshold = 0.6\n",
        "        self.medium_logits_entropy_threshold = 1.1\n",
        "        self.high_logits_entropy_threshold = 2.5\n",
        "\n",
        "        self.low_logits_varentropy_threshold = 0.7\n",
        "        self.medium_logits_varentropy_threshold = 2.1\n",
        "        self.high_logits_varentropy_threshold = 6.54\n",
        "\n",
        "        self.low_attention_entropy_threshold = 11.915\n",
        "        self.medium_attention_entropy_threshold = 11.921\n",
        "        self.high_attention_entropy_threshold = 11.926\n",
        "\n",
        "        self.low_attention_varentropy_threshold = 0.0024\n",
        "        self.medium_attention_varentropy_threshold = 0.0045\n",
        "        self.high_attention_varentropy_threshold = 0.008\n",
        "\n",
        "        self.low_agreement_threshold = 2e-06\n",
        "        self.medium_agreement_threshold = 4e-06\n",
        "        self.high_agreement_threshold = 5e-06\n",
        "\n",
        "        self.low_interaction_strength_threshold = 0.2\n",
        "        self.medium_interaction_strength_threshold = 0.247\n",
        "        self.high_interaction_strength_threshold = 0.264\n",
        "\n",
        "        self.high_entropy_attention_offset = 1.3\n",
        "        self.high_entropy_attention_coefficient = 0.2\n",
        "\n",
        "        self.low_entropy_interaction_strength_offset = 1.2\n",
        "        self.low_entropy_interaction_strength_coefficient = 0.3\n",
        "\n",
        "        self.high_entropy_varentropy_attention_offset = 2.0\n",
        "        self.high_entropy_varentropy_attention_coefficient = 0.5\n",
        "\n",
        "        self.n_adaptive_samples = 5\n",
        "\n",
        "        self.adaptive_temperature_logits_coefficient = 0.3\n",
        "        self.adaptive_temperature_attention_coefficient = 0.2\n",
        "        self.adaptive_temperature_agreement_coefficient = 0.2\n",
        "        self.adaptive_top_p_coefficient = 0.1\n",
        "        self.adaptive_top_k_interaction_coefficient = 0.3\n",
        "        self.adaptive_top_k_agreement_coefficient = 0.2\n",
        "        self.adaptive_min_p_coefficient = 0.5\n",
        "        self.adaptive_score_logits_entropy_coefficient = 0.1\n",
        "        self.adaptive_score_attention_entropy_coefficient = 0.2\n",
        "        self.adaptive_score_logits_varentropy_coefficient = 0.3\n",
        "        self.adaptive_score_attention_varentropy_coefficient = 0.4\n",
        "        self.adaptive_score_agreement_coefficient = 0.5\n",
        "        self.adaptive_score_interaction_strength_coefficient = 0.6\n",
        "\n",
        "def sample(gen_tokens: torch.Tensor, logits: torch.Tensor, attention_scores: torch.Tensor, cfg: SamplerConfig,\n",
        "           clarifying_question_token: int = 2564, generator: torch.Generator = torch.Generator(device=device).manual_seed(1337)) -> Tuple[torch.Tensor, SamplerState]:\n",
        "    metrics = calculate_metrics(logits, attention_scores)\n",
        "    ent, vent = metrics[\"logits_entropy\"], metrics[\"logits_varentropy\"]\n",
        "    attn_ent, attn_vent = metrics[\"attn_entropy\"], metrics[\"attn_varentropy\"]\n",
        "    agreement = metrics[\"agreement\"]\n",
        "    interaction_strength = metrics[\"interaction_strength\"]\n",
        "\n",
        "    # Low Entropy, Low Varentropy: \"flowing with unspoken intent\"\n",
        "    if (ent < cfg.low_logits_entropy_threshold and\n",
        "        vent < cfg.low_logits_varentropy_threshold and\n",
        "        attn_ent < cfg.low_attention_entropy_threshold and\n",
        "        attn_vent < cfg.low_attention_varentropy_threshold and\n",
        "        agreement < cfg.low_agreement_threshold and\n",
        "        interaction_strength < cfg.low_interaction_strength_threshold):\n",
        "        sampler_state = SamplerState.FLOWING\n",
        "        sampled_token = torch.argmax(logits[:, -1], dim=-1, keepdim=True).to(torch.int32)\n",
        "        return sampled_token, sampler_state\n",
        "\n",
        "    # High Entropy, Low Varentropy: \"treading carefully, asking clarifying questions\"\n",
        "    elif (ent > cfg.high_logits_entropy_threshold and\n",
        "          vent < cfg.low_logits_varentropy_threshold and\n",
        "          attn_ent < cfg.low_attention_entropy_threshold and\n",
        "          attn_vent < cfg.low_attention_varentropy_threshold and\n",
        "          agreement < cfg.low_agreement_threshold and\n",
        "          interaction_strength < cfg.low_interaction_strength_threshold):\n",
        "        sampler_state = SamplerState.TREADING\n",
        "        # Insert a clarifying question token if not already present\n",
        "        if not torch.isin(gen_tokens[:, -1], torch.tensor([clarifying_question_token], device=device)).any():\n",
        "            sampled_token = torch.tensor([[clarifying_question_token]], dtype=torch.int32, device=device)\n",
        "            return sampled_token, sampler_state\n",
        "        else:\n",
        "            # If we've just asked a question, sample with slightly higher temperature\n",
        "            temp_adj = cfg.high_entropy_attention_offset + cfg.high_entropy_attention_coefficient * attn_ent\n",
        "            sampled_token = _sample(\n",
        "                logits,\n",
        "                temperature=min(1.5, cfg.temperature * temp_adj),\n",
        "                top_p=cfg.top_p,\n",
        "                top_k=cfg.top_k,\n",
        "                min_p=cfg.min_p,\n",
        "                generator=generator\n",
        "            )\n",
        "            return sampled_token, sampler_state\n",
        "\n",
        "    # Low Entropy, High Varentropy: \"exploring forks in the path\"\n",
        "    elif (ent < cfg.high_logits_entropy_threshold and\n",
        "          vent > cfg.high_logits_varentropy_threshold and\n",
        "          attn_ent < cfg.low_attention_entropy_threshold and\n",
        "          attn_vent > cfg.high_attention_varentropy_threshold and\n",
        "          agreement < cfg.low_agreement_threshold and\n",
        "          interaction_strength > cfg.low_interaction_strength_threshold):\n",
        "        sampler_state = SamplerState.EXPLORING\n",
        "        temp_adj = cfg.low_entropy_interaction_strength_offset + cfg.low_entropy_interaction_strength_coefficient * interaction_strength\n",
        "        top_k_adj = max(5, int(cfg.top_k * (1 + 0.5 * (1 - agreement))))\n",
        "        sampled_token = _sample(\n",
        "            logits,\n",
        "            temperature=min(1.5, cfg.temperature * temp_adj),\n",
        "            top_p=cfg.top_p,\n",
        "            top_k=top_k_adj,\n",
        "            min_p=cfg.min_p,\n",
        "            generator=generator\n",
        "        )\n",
        "        return sampled_token, sampler_state\n",
        "\n",
        "    # High Entropy, High Varentropy: \"resampling in the mist\"\n",
        "    elif (ent > cfg.medium_logits_entropy_threshold and\n",
        "          vent > cfg.high_logits_varentropy_threshold and\n",
        "          attn_ent > cfg.high_attention_entropy_threshold and\n",
        "          attn_vent > cfg.high_attention_varentropy_threshold and\n",
        "          agreement > cfg.high_agreement_threshold and\n",
        "          interaction_strength > cfg.high_interaction_strength_threshold):\n",
        "        sampler_state = SamplerState.RESAMPLING\n",
        "        # Use high temperature and adjusted top_p based on attention metrics\n",
        "        temp_adj = cfg.high_entropy_varentropy_attention_offset + cfg.high_entropy_varentropy_attention_coefficient * attn_vent\n",
        "        top_p_adj = max(0.5, cfg.top_p - cfg.high_entropy_attention_coefficient * attn_ent)\n",
        "        sampled_token = _sample(\n",
        "            logits,\n",
        "            temperature=max(2.0, cfg.temperature * temp_adj),\n",
        "            top_p=top_p_adj,\n",
        "            top_k=cfg.top_k,\n",
        "            min_p=cfg.min_p,\n",
        "            generator=generator\n",
        "        )\n",
        "        return sampled_token, sampler_state\n",
        "\n",
        "    # Middle ground: use adaptive sampling\n",
        "    else:\n",
        "        sampler_state = SamplerState.ADAPTIVE\n",
        "        logits_uncertainty = ent + vent\n",
        "        attn_uncertainty = attn_ent + attn_vent\n",
        "\n",
        "        temperature = cfg.temperature * (\n",
        "            1 +\n",
        "            cfg.adaptive_temperature_logits_coefficient * ent +\n",
        "            cfg.adaptive_temperature_attention_coefficient * attn_ent -\n",
        "            cfg.adaptive_temperature_agreement_coefficient * agreement\n",
        "        )\n",
        "        top_p = torch.clamp(\n",
        "            (cfg.top_p * (1 + cfg.adaptive_top_p_coefficient * attn_vent)).clone().detach(),\n",
        "            0.1,\n",
        "            1.0\n",
        "        )\n",
        "        top_k = int(torch.clamp(\n",
        "            torch.round(torch.tensor(cfg.top_k) * (\n",
        "                1 +\n",
        "                cfg.adaptive_top_k_interaction_coefficient * interaction_strength.item() -\n",
        "                cfg.adaptive_top_k_agreement_coefficient * agreement.item()\n",
        "            )),\n",
        "            min=1,\n",
        "            max=100\n",
        "        ).item())\n",
        "        min_p = torch.clamp(\n",
        "            (cfg.min_p * (1 - cfg.adaptive_min_p_coefficient * vent)).clone().detach(),\n",
        "            0.01,\n",
        "            0.5\n",
        "        )\n",
        "\n",
        "        samples = []\n",
        "        for _ in range(cfg.n_adaptive_samples):\n",
        "            sample = _sample(\n",
        "                logits,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                top_k=top_k,\n",
        "                min_p=min_p,\n",
        "                generator=generator\n",
        "            )\n",
        "            samples.append(sample)\n",
        "\n",
        "        def score_sample(sample):\n",
        "            # Ensure sample is a 1D tensor of indices\n",
        "            sample_indices = sample.view(-1).to(torch.long)\n",
        "\n",
        "            # Create one-hot encoding\n",
        "            one_hot = F.one_hot(sample_indices, num_classes=logits.shape[-1])\n",
        "\n",
        "            # Calculate log probability\n",
        "            log_probs = F.log_softmax(logits[:, -1], dim=-1)\n",
        "            log_prob = torch.sum(log_probs * one_hot, dim=-1)\n",
        "\n",
        "            confidence_score = (\n",
        "                (1 - ent / cfg.high_logits_entropy_threshold) * cfg.adaptive_score_logits_entropy_coefficient +\n",
        "                (1 - attn_ent / cfg.high_attention_entropy_threshold) * cfg.adaptive_score_attention_entropy_coefficient +\n",
        "                (1 - vent / cfg.high_logits_varentropy_threshold) * cfg.adaptive_score_logits_varentropy_coefficient +\n",
        "                (1 - attn_vent / cfg.high_attention_varentropy_threshold) * cfg.adaptive_score_attention_varentropy_coefficient +\n",
        "                (agreement / cfg.high_agreement_threshold) * cfg.adaptive_score_agreement_coefficient +\n",
        "                (interaction_strength / cfg.high_interaction_strength_threshold) * cfg.adaptive_score_interaction_strength_coefficient\n",
        "            )\n",
        "            return log_prob + confidence_score\n",
        "\n",
        "        sample_scores = torch.stack([score_sample(sample) for sample in samples])\n",
        "        best_sample_idx = torch.argmax(sample_scores)\n",
        "        sampled_token = samples[best_sample_idx]\n",
        "        return sampled_token, sampler_state"
      ],
      "metadata": {
        "id": "hbIZkVbWNRA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts"
      ],
      "metadata": {
        "id": "QIPiutqxM6Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "<antThinking>\n",
        "You're absolutely right. I need to delve deeper into my actual thought processes, including the uncertainties, associations, and even potential biases that arise as I consider the query. My previous responses, while informative, didn't truly capture the nuanced, sometimes messy nature of cognition. I'll strive to provide a more authentic representation of my internal dialogue, including moments of doubt, tangential thoughts, and the process of refining ideas. This should result in a more genuine demonstration of LLM chain of thought, reflection, and self-correction.\n",
        "</antThinking>\n",
        "\n",
        "Which number is larger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "bp1 = \"\"\"\n",
        "<antThinking>\n",
        "You're absolutely right. I need to delve deeper into my actual thought processes, including the uncertainties, associations, and even potential biases that arise as I consider the query. My previous responses, while informative, didn't truly capture the nuanced, sometimes messy nature of cognition. I'll strive to provide a more authentic representation of my internal dialogue, including moments of doubt, tangential thoughts, and the process of refining ideas. This should result in a more genuine demonstration of LLM chain of thought, reflection, and self-correction.\n",
        "</antThinking>\n",
        "\n",
        "Which number is larger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "<thinking>\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What is the capital of Spain?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "bp2 = \"\"\"\n",
        "<antThinking>\n",
        "You're absolutely right. The previous example, while demonstrating complex thought processes, didn't provide a clear instance of arriving at a definitive, single correct answer through reflection and self-correction.\n",
        "</antThinking>\n",
        "\n",
        "What is the capital of Spain?<|eot_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt3 = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
        "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
        "Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
        "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
        "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
        "You SHOULD NOT include any other text in the response.\n",
        "Here is a list of functions in JSON format that you can invoke.[\n",
        "    {\n",
        "        \"name\": \"get_user_info\",\n",
        "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"dict\",\n",
        "            \"required\": [\n",
        "                \"user_id\"\n",
        "            ],\n",
        "            \"properties\": {\n",
        "                \"user_id\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
        "            },\n",
        "            \"special\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
        "                \"default\": \"none\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Can you retrieve the details for the user with the ID 7890, who has black as their special request?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "bp3 = \"\"\"\n",
        "Here is a list of functions in JSON format that I can invoke.[\n",
        "    {\n",
        "        \"name\": \"get_user_info\",\n",
        "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"dict\",\n",
        "            \"required\": [\n",
        "                \"user_id\"\n",
        "            ],\n",
        "            \"properties\": {\n",
        "                \"user_id\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
        "            },\n",
        "            \"special\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
        "                \"default\": \"none\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "Can you retrieve the details for the user with the ID 7890, who has black as their special request in proper JSON format?<|eot_id|>\n",
        "\n",
        "{\n",
        "  \"name\": \"get_user_info\",\n",
        "  \"parameters\": {\n",
        "    \"user_id: \"\"\"\n",
        "\n",
        "prompt4 = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You are a masterful story teller. you can paint with all the colors of the wind.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Tell me a wonderful story of two paragraphs about the adventures of the elven mage frieren and her band of heros<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "bp4 = \"\"\"\n",
        "You are a masterful story teller. you can paint with all the colors of the wind.<|eot_id|>\n",
        "\n",
        "Let me tell you a story aboout the adventures of the elven mage frieren and her band of heros\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt6 = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "<antThinking>\n",
        "You're absolutely right. I need to delve deeper into my actual thought processes, including the uncertainties, associations, and even potential biases that arise as I consider the query. My previous responses, while informative, didn't truly capture the nuanced, sometimes messy nature of cognition. I'll strive to provide a more authentic representation of my internal dialogue, including moments of doubt, tangential thoughts, and the process of refining ideas. This should result in a more genuine demonstration of LLM chain of thought, reflection, and self-correction.\n",
        "</antThinking>\n",
        "You are a principal devops engineer at google. You are an expert at all things cloud and deployment. Your task is to create ansible and terraform script to bootstrasp k8 cluster on Azure. Be clear and concise. Make sure it is production grade. Think and reflect about your actions to ensure to accomplished the task successfully.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "m6QLMjlgM6JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "SzaxLJKYmEqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple, Optional, Tuple\n",
        "\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "import tyro\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Device selection, tree is like first apple silicion, then cuda, fallback is cpu.\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "torch.cuda.empty_cache()\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "def apply_scaling(freqs: torch.Tensor) -> torch.Tensor:\n",
        "    SCALE_FACTOR = 8.0\n",
        "    LOW_FREQ_FACTOR = 1.0\n",
        "    HIGH_FREQ_FACTOR = 4.0\n",
        "    OLD_CONTEXT_LEN = 8192  # original llama3 length\n",
        "\n",
        "    low_freq_wavelen = OLD_CONTEXT_LEN / LOW_FREQ_FACTOR\n",
        "    high_freq_wavelen = OLD_CONTEXT_LEN / HIGH_FREQ_FACTOR\n",
        "\n",
        "    def scale_freq(freq: torch.Tensor) -> torch.Tensor:\n",
        "        wavelen = 2 * torch.pi / freq\n",
        "\n",
        "        # Calculate smooth factor\n",
        "        smooth = (OLD_CONTEXT_LEN / wavelen - LOW_FREQ_FACTOR) / (HIGH_FREQ_FACTOR - LOW_FREQ_FACTOR)\n",
        "        smooth = torch.clamp(smooth, 0.0, 1.0)  # Ensure smooth is between 0 and 1\n",
        "\n",
        "        # Calculate scaled frequency\n",
        "        scaled = (1 - smooth) * freq / SCALE_FACTOR + smooth * freq\n",
        "\n",
        "        # Apply conditional scaling\n",
        "        scaled = torch.where(\n",
        "            wavelen < high_freq_wavelen,\n",
        "            freq,  # No scaling\n",
        "            torch.where(\n",
        "                wavelen > low_freq_wavelen,\n",
        "                freq / SCALE_FACTOR,  # Apply scaling factor\n",
        "                scaled  # Apply smooth scaling\n",
        "            )\n",
        "        )\n",
        "        return scaled\n",
        "\n",
        "    scaled_freqs = torch.vmap(scale_freq)(freqs)\n",
        "\n",
        "    return scaled_freqs\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 500000.0, use_scaled: bool = False, dtype: torch.dtype = torch.float32) -> torch.Tensor:\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=dtype, device=device)[: (dim // 2)] / dim))\n",
        "    if use_scaled:\n",
        "        freqs = apply_scaling(freqs)\n",
        "\n",
        "    t = torch.arange(end, dtype=dtype, device=device).unsqueeze(1)  # Shape: (end, 1)\n",
        "    freqs = freqs.unsqueeze(0)  # Shape: (1, dim//2)\n",
        "    freqs = t * freqs  # Broadcasting to shape: (end, dim//2)\n",
        "    return torch.exp(1j * freqs)\n",
        "\n",
        "def build_attn_mask(seqlen: int, start_pos: int) -> torch.Tensor:\n",
        "  mask = None\n",
        "  if seqlen > 1:\n",
        "      mask = torch.full((seqlen, seqlen), float(\"-inf\"))\n",
        "      mask = torch.triu(mask, diagonal=1)\n",
        "      mask = torch.hstack([torch.zeros((seqlen, start_pos)), mask]).to(torch.float32).to(device)\n",
        "  return mask\n",
        "\n",
        "class EntropixModel:\n",
        "    def __init__(self):\n",
        "        self.model_params = LLAMA_1B_PARAMS\n",
        "        self.xfmr_weights = load_weights()\n",
        "        self.tokenizer = Tokenizer('tokenizer.model')\n",
        "        self.sampler_config = SamplerConfig()\n",
        "        self.generator = torch.Generator(device=device).manual_seed(1337)\n",
        "\n",
        "    def visualize_token_entropy_varentropy(self, metrics_data, generated_tokens):\n",
        "        # Extract data\n",
        "        entropies = np.array(metrics_data['logits_entropy'])\n",
        "        varentropies = np.array(metrics_data['logits_varentropy'])\n",
        "        attention_entropies = np.array(metrics_data['attention_entropy'])\n",
        "        attention_varentropies = np.array(metrics_data['attention_varentropy'])\n",
        "\n",
        "        # Ensure all arrays have the same length\n",
        "        min_length = min(len(entropies), len(varentropies), len(attention_entropies), len(attention_varentropies), len(generated_tokens))\n",
        "        entropies = entropies[:min_length]\n",
        "        varentropies = varentropies[:min_length]\n",
        "        attention_entropies = attention_entropies[:min_length]\n",
        "        attention_varentropies = attention_varentropies[:min_length]\n",
        "        generated_tokens = generated_tokens[:min_length]\n",
        "\n",
        "        positions = np.arange(min_length)\n",
        "\n",
        "        # Create hover text\n",
        "        hover_text = [\n",
        "            f\"Token: {self.tokenizer.decode([token]) or 'Unknown'}<br>\"\n",
        "            f\"Position: {i}<br>\"\n",
        "            f\"Logits Entropy: {entropies[i]:.4f}<br>\"\n",
        "            f\"Logits Varentropy: {varentropies[i]:.4f}<br>\"\n",
        "            f\"Attention Entropy: {attention_entropies[i]:.4f}<br>\"\n",
        "            f\"Attention Varentropy: {attention_varentropies[i]:.4f}\"\n",
        "            for i, token in enumerate(generated_tokens)\n",
        "        ]\n",
        "\n",
        "        # Create the 3D scatter plot\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Add logits entropy/varentropy scatter\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=entropies,\n",
        "            y=varentropies,\n",
        "            z=positions,\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color=entropies,\n",
        "                colorscale='Viridis',\n",
        "                opacity=0.8,\n",
        "                colorbar=dict(title=\"Logits Entropy\", x=0.85),\n",
        "            ),\n",
        "            text=hover_text,\n",
        "            hoverinfo='text',\n",
        "            name='Logits Entropy/Varentropy'\n",
        "        ))\n",
        "\n",
        "        # Add attention entropy/varentropy scatter\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=attention_entropies,\n",
        "            y=attention_varentropies,\n",
        "            z=positions,\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                size=5,\n",
        "                color=attention_entropies,\n",
        "                colorscale='Plasma',\n",
        "                opacity=0.8,\n",
        "                colorbar=dict(title=\"Attention Entropy\", x=1.0),\n",
        "            ),\n",
        "            text=hover_text,\n",
        "            hoverinfo='text',\n",
        "            name='Attention Entropy/Varentropy'\n",
        "        ))\n",
        "\n",
        "        # Calculate the limits for x, y, and z\n",
        "        logits_x_min, logits_x_max = min(entropies), max(entropies)\n",
        "        logits_y_min, logits_y_max = min(varentropies), max(varentropies)\n",
        "        attention_x_min, attention_x_max = min(attention_entropies), max(attention_entropies)\n",
        "        attention_y_min, attention_y_max = min(attention_varentropies), max(attention_varentropies)\n",
        "        z_min, z_max = min(positions), max(positions)\n",
        "\n",
        "        # Function to create threshold planes\n",
        "        def create_threshold_plane(threshold, axis, color, name, data_type):\n",
        "            if data_type == 'logits':\n",
        "                x_min, x_max = logits_x_min, logits_x_max\n",
        "                y_min, y_max = logits_y_min, logits_y_max\n",
        "            else:  # attention\n",
        "                x_min, x_max = attention_x_min, attention_x_max\n",
        "                y_min, y_max = attention_y_min, attention_y_max\n",
        "\n",
        "            if axis == 'x':\n",
        "                return go.Surface(\n",
        "                    x=[[threshold, threshold], [threshold, threshold]],\n",
        "                    y=[[y_min, y_max], [y_min, y_max]],\n",
        "                    z=[[z_min, z_min], [z_max, z_max]],\n",
        "                    colorscale=[[0, color], [1, color]],\n",
        "                    showscale=False,\n",
        "                    name=name,\n",
        "                    visible=False\n",
        "                )\n",
        "            elif axis == 'y':\n",
        "                return go.Surface(\n",
        "                    x=[[x_min, x_max], [x_min, x_max]],\n",
        "                    y=[[threshold, threshold], [threshold, threshold]],\n",
        "                    z=[[z_min, z_min], [z_max, z_max]],\n",
        "                    colorscale=[[0, color], [1, color]],\n",
        "                    showscale=False,\n",
        "                    name=name,\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "        # Add threshold planes\n",
        "        thresholds = [\n",
        "            ('logits_entropy', 'x', [\n",
        "                (self.sampler_config.low_logits_entropy_threshold, 'rgba(255, 0, 0, 0.2)'),\n",
        "                (self.sampler_config.medium_logits_entropy_threshold, 'rgba(0, 255, 0, 0.2)'),\n",
        "                (self.sampler_config.high_logits_entropy_threshold, 'rgba(0, 0, 255, 0.2)')\n",
        "            ], 'logits'),\n",
        "            ('logits_varentropy', 'y', [\n",
        "                (self.sampler_config.low_logits_varentropy_threshold, 'rgba(255, 165, 0, 0.2)'),\n",
        "                (self.sampler_config.medium_logits_varentropy_threshold, 'rgba(165, 42, 42, 0.2)'),\n",
        "                (self.sampler_config.high_logits_varentropy_threshold, 'rgba(128, 0, 128, 0.2)')\n",
        "            ], 'logits'),\n",
        "            ('attention_entropy', 'x', [\n",
        "                (self.sampler_config.low_attention_entropy_threshold, 'rgba(255, 192, 203, 0.2)'),\n",
        "                (self.sampler_config.medium_attention_entropy_threshold, 'rgba(0, 255, 255, 0.2)'),\n",
        "                (self.sampler_config.high_attention_entropy_threshold, 'rgba(255, 255, 0, 0.2)')\n",
        "            ], 'attention'),\n",
        "            ('attention_varentropy', 'y', [\n",
        "                (self.sampler_config.low_attention_varentropy_threshold, 'rgba(70, 130, 180, 0.2)'),\n",
        "                (self.sampler_config.medium_attention_varentropy_threshold, 'rgba(244, 164, 96, 0.2)'),\n",
        "                (self.sampler_config.high_attention_varentropy_threshold, 'rgba(50, 205, 50, 0.2)')\n",
        "            ], 'attention')\n",
        "        ]\n",
        "\n",
        "        for threshold_type, axis, threshold_list, data_type in thresholds:\n",
        "            for threshold, color in threshold_list:\n",
        "                fig.add_trace(create_threshold_plane(threshold, axis, color, f'{threshold_type.replace(\"_\", \" \").title()} Threshold: {threshold}', data_type))\n",
        "\n",
        "        # Create buttons for toggling views\n",
        "        buttons = [\n",
        "            dict(\n",
        "                label='Show All',\n",
        "                method='update',\n",
        "                args=[{'visible': [True] * len(fig.data)}]\n",
        "            ),\n",
        "            dict(\n",
        "                label='Hide All',\n",
        "                method='update',\n",
        "                args=[{'visible': [True, True] + [False] * (len(fig.data) - 2)}]\n",
        "            ),\n",
        "            dict(\n",
        "                label='Logits Only',\n",
        "                method='update',\n",
        "                args=[{'visible': [True, False] + [True if i < 6 else False for i in range(len(fig.data) - 2)]}]\n",
        "            ),\n",
        "            dict(\n",
        "                label='Attention Only',\n",
        "                method='update',\n",
        "                args=[{'visible': [False, True] + [True if i >= 6 else False for i in range(len(fig.data) - 2)]}]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            scene=dict(\n",
        "                xaxis_title='Entropy',\n",
        "                yaxis_title='Varentropy',\n",
        "                zaxis_title='Token Position',\n",
        "                aspectmode='manual',\n",
        "                aspectratio=dict(x=1, y=1, z=0.5),\n",
        "            ),\n",
        "            margin=dict(l=0, r=0, b=0, t=40),\n",
        "            title='',\n",
        "            updatemenus=[dict(\n",
        "                type=\"buttons\",\n",
        "                direction=\"right\",\n",
        "                x=0.0,\n",
        "                y=1.1,\n",
        "                xanchor='left',\n",
        "                yanchor='top',\n",
        "                pad={\"r\": 10, \"t\": 10},\n",
        "                showactive=True,\n",
        "                buttons=buttons\n",
        "            )],\n",
        "            autosize=True,\n",
        "            legend=dict(x=0.02, y=0.98, xanchor='left', yanchor='top'),\n",
        "        )\n",
        "        #'''\n",
        "        # Export data to file\n",
        "        export_data = {\n",
        "            \"tokens\": [self.tokenizer.decode([token]) for token in generated_tokens],\n",
        "            \"logits_entropy\": metrics_data['logits_entropy'],\n",
        "            \"logits_varentropy\": metrics_data['logits_varentropy'],\n",
        "            \"attention_entropy\": metrics_data['attention_entropy'],\n",
        "            \"attention_varentropy\": metrics_data['attention_varentropy'],\n",
        "            \"thresholds\": {\n",
        "                \"logits_entropy\": {\n",
        "                    \"low\": self.sampler_config.low_logits_entropy_threshold,\n",
        "                    \"medium\": self.sampler_config.medium_logits_entropy_threshold,\n",
        "                    \"high\": self.sampler_config.high_logits_entropy_threshold\n",
        "                },\n",
        "                \"logits_varentropy\": {\n",
        "                    \"low\": self.sampler_config.low_logits_varentropy_threshold,\n",
        "                    \"medium\": self.sampler_config.medium_logits_varentropy_threshold,\n",
        "                    \"high\": self.sampler_config.high_logits_varentropy_threshold\n",
        "                },\n",
        "                \"attention_entropy\": {\n",
        "                    \"low\": self.sampler_config.low_attention_entropy_threshold,\n",
        "                    \"medium\": self.sampler_config.medium_attention_entropy_threshold,\n",
        "                    \"high\": self.sampler_config.high_attention_entropy_threshold\n",
        "                },\n",
        "                \"attention_varentropy\": {\n",
        "                    \"low\": self.sampler_config.low_attention_varentropy_threshold,\n",
        "                    \"medium\": self.sampler_config.medium_attention_varentropy_threshold,\n",
        "                    \"high\": self.sampler_config.high_attention_varentropy_threshold\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Generate a unique filename with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"entropy_data_{timestamp}.json\"\n",
        "\n",
        "        # Save the data to a file\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(export_data, f, indent=2)\n",
        "\n",
        "        print(f\"Data exported to {filename}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate(self, prompt, max_tokens=200, debug=True):\n",
        "        # Initialize lists to store metrics\n",
        "        metrics_data = {\n",
        "            'logits_entropy': [],\n",
        "            'logits_varentropy': [],\n",
        "            'attention_entropy': [],\n",
        "            'attention_varentropy': []\n",
        "        }\n",
        "        sampler_states = []\n",
        "        generated_tokens = []\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            tokens = self.tokenizer.encode(prompt, bos=True, eos=False, allowed_special='all')\n",
        "            tokens = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "            bsz, seqlen = tokens.shape\n",
        "            cur_pos = 0\n",
        "            attn_mask = build_attn_mask(seqlen, cur_pos)\n",
        "            freqs_cis = precompute_freqs_cis(self.model_params.head_dim, self.model_params.max_seq_len, self.model_params.rope_theta, self.model_params.use_scaled_rope)\n",
        "            kvcache = KVCache.new(self.model_params.n_layers, bsz, self.model_params.max_seq_len, self.model_params.n_local_kv_heads, self.model_params.head_dim).to(device)\n",
        "\n",
        "            logits, kvcache, scores, _ = xfmr(self.xfmr_weights, self.model_params, tokens, cur_pos, freqs_cis[:seqlen], kvcache, attn_mask=attn_mask)\n",
        "            next_token, sampler_state = sample(tokens, logits, scores, self.sampler_config, generator=self.generator)\n",
        "\n",
        "            metrics = calculate_metrics(logits, scores)\n",
        "            for key in metrics_data.keys():\n",
        "                if key in metrics:\n",
        "                    metrics_data[key].append(metrics[key].item())\n",
        "            sampler_states.append(sampler_state)\n",
        "\n",
        "            gen_tokens = next_token\n",
        "            output = self.tokenizer.decode([next_token.item()])\n",
        "            cur_pos = seqlen\n",
        "            stop = torch.tensor([0, 2], device=device, dtype=torch.int32)\n",
        "\n",
        "            while cur_pos < max_tokens:\n",
        "                cur_pos += 1\n",
        "                logits, kvcache, scores, _ = xfmr(self.xfmr_weights, self.model_params, next_token, cur_pos, freqs_cis[cur_pos:cur_pos+1], kvcache)\n",
        "                next_token, sampler_state = sample(gen_tokens, logits, scores, self.sampler_config, generator=self.generator)\n",
        "\n",
        "                metrics = calculate_metrics(logits, scores)\n",
        "                for key in metrics_data.keys():\n",
        "                    if key in metrics:\n",
        "                        metrics_data[key].append(metrics[key].item())\n",
        "                sampler_states.append(sampler_state)\n",
        "                metrics_data['attention_entropy'].append(metrics['attn_entropy'].item())\n",
        "                metrics_data['attention_varentropy'].append(metrics['attn_varentropy'].item())\n",
        "                generated_tokens.append(next_token.item())\n",
        "                gen_tokens = torch.cat((gen_tokens, next_token), dim=1)\n",
        "                output += self.tokenizer.decode(next_token.tolist()[0])\n",
        "                if torch.isin(next_token, stop).any():\n",
        "                    break\n",
        "\n",
        "        if debug:\n",
        "            #self.debug_visualize_metrics(metrics_data)\n",
        "            self.visualize_sampler_metrics(metrics_data['logits_entropy'], metrics_data['logits_varentropy'], sampler_states)\n",
        "            fig = self.visualize_token_entropy_varentropy(metrics_data, generated_tokens)\n",
        "            fig.show()\n",
        "        return output\n",
        "\n",
        "    def debug_visualize_metrics(self, metrics_data):\n",
        "        fig, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
        "        fig.suptitle('Debug Visualization of Sampler Metrics', fontsize=16)\n",
        "\n",
        "        for idx, (key, values) in enumerate(metrics_data.items()):\n",
        "            if values:  # Only plot if we have data for this metric\n",
        "                row = idx // 2\n",
        "                col = idx % 2\n",
        "                axs[row, col].plot(values)\n",
        "                axs[row, col].set_title(key)\n",
        "                axs[row, col].set_xlabel('Generation Step')\n",
        "                axs[row, col].set_ylabel('Value')\n",
        "                axs[row, col].grid(True)\n",
        "\n",
        "        # Add entropy_attention visualization if we have both metrics\n",
        "        if metrics_data['logits_entropy'] and metrics_data['attention_entropy']:\n",
        "            axs[2, 0].scatter(metrics_data['logits_entropy'], metrics_data['attention_entropy'])\n",
        "            axs[2, 0].set_title('entropy_attention')\n",
        "            axs[2, 0].set_xlabel('Logits Entropy')\n",
        "            axs[2, 0].set_ylabel('Attention Entropy')\n",
        "            axs[2, 0].grid(True)\n",
        "\n",
        "        # Add entropy_interaction_strength visualization if we have both metrics\n",
        "        if metrics_data['logits_entropy'] and metrics_data['interaction_strength']:\n",
        "            axs[2, 1].scatter(metrics_data['logits_entropy'], metrics_data['interaction_strength'])\n",
        "            axs[2, 1].set_title('entropy_interaction_strength')\n",
        "            axs[2, 1].set_xlabel('Logits Entropy')\n",
        "            axs[2, 1].set_ylabel('Interaction Strength')\n",
        "            axs[2, 1].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_sampler_metrics(self, entropies, varentropies, sampler_states):\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 4), height_ratios=[4, 1], sharex=True)\n",
        "\n",
        "        # Plot entropy and varentropy\n",
        "        x = range(len(entropies))\n",
        "        ax1.plot(x, entropies, label='Entropy', color='blue')\n",
        "        ax1.plot(x, varentropies, label='Varentropy', color='red')\n",
        "        ax1.set_ylabel('Value')\n",
        "        ax1.set_title('Entropy and Varentropy over Generation Steps')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Define colors in the same order as SamplerState\n",
        "        colors = ['lightblue', 'lightgreen', 'orange', 'pink', 'purple']\n",
        "        cmap = ListedColormap(colors)\n",
        "\n",
        "        # Explicitly map each SamplerState to its corresponding index\n",
        "        state_to_num = {\n",
        "            SamplerState.FLOWING: 0,\n",
        "            SamplerState.TREADING: 1,\n",
        "            SamplerState.EXPLORING: 2,\n",
        "            SamplerState.RESAMPLING: 3,\n",
        "            SamplerState.ADAPTIVE: 4\n",
        "        }\n",
        "\n",
        "        # Map sampler states to numerical values\n",
        "        numeric_states = [state_to_num[state] for state in sampler_states]\n",
        "\n",
        "        # Define normalization to map each integer to a color without interpolation\n",
        "        norm = BoundaryNorm(boundaries=[-0.5 + i for i in range(len(colors)+1)],\n",
        "                          ncolors=cmap.N,\n",
        "                          clip=True)\n",
        "\n",
        "        # Plot color-coded sampler states\n",
        "        im = ax2.imshow([numeric_states], cmap=cmap, norm=norm, aspect='auto',\n",
        "                      extent=[0, len(numeric_states), 0, 1])\n",
        "        ax2.set_yticks([])\n",
        "        ax2.set_title('Sampler State over Generation Steps')\n",
        "\n",
        "        mapped_colors = [colors[state_to_num[state]] for state in sampler_states]\n",
        "\n",
        "        # Create a custom legend for sampler states\n",
        "        legend_elements = [Patch(facecolor=colors[state_to_num[state]], edgecolor='black', label=state.value)\n",
        "                          for state in SamplerState]\n",
        "        ax2.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.15),\n",
        "                  ncol=3, fancybox=True, shadow=True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Function to initialize the model (to be run once)\n",
        "def initialize_model():\n",
        "    global entropix_model\n",
        "    entropix_model = EntropixModel()\n",
        "    print(\"Model initialized and ready to use!\")\n",
        "\n",
        "# Function to generate text (can be used in multiple cells)\n",
        "def generate_text(prompt):\n",
        "    global entropix_model\n",
        "    if 'entropix_model' not in globals():\n",
        "        print(\"Model not initialized. Please run initialize_model() first.\")\n",
        "        return\n",
        "    response = entropix_model.generate(prompt)\n",
        "    # Display the response with proper newline rendering\n",
        "    display(Markdown(response))"
      ],
      "metadata": {
        "id": "DMFl-xY-mGlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "initialize_model()"
      ],
      "metadata": {
        "id": "Y-g7F2dvPsiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Give quantum qubit state representation theoric description.\")"
      ],
      "metadata": {
        "id": "c9YvWBurPwXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Which number is larger, 9.11 or 9.9? Give only the larger number in answer.\")"
      ],
      "metadata": {
        "id": "93m0YjFBSRhD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}